package main

import (
	"bytes"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"time"

	"golang.org/x/net/html"
)

// URLInfo å­˜å‚¨URLä¿¡æ¯
type URLInfo struct {
	URL   string
	Depth int
}

// Crawler çˆ¬è™«ç»“æ„ä½“
type Crawler struct {
	startURL    string
	maxDepth    int
	visited     sync.Map
	results     chan string
	workerCount int
	delay       time.Duration
	domain      string
	outputDir   string
}

// NewCrawler åˆ›å»ºæ–°çš„çˆ¬è™«å®ä¾‹
func NewCrawler(startURL string, maxDepth, workers int, delay time.Duration) (*Crawler, error) {
	parsed, err := url.Parse(startURL)
	if err != nil {
		return nil, err
	}

	// åˆ›å»ºè¾“å‡ºç›®å½•
	outputDir := fmt.Sprintf("crawled_%s_%d",
		strings.ReplaceAll(parsed.Host, ".", "_"),
		time.Now().Unix())

	if err := os.MkdirAll(outputDir, 0755); err != nil {
		return nil, err
	}

	return &Crawler{
		startURL:    startURL,
		maxDepth:    maxDepth,
		results:     make(chan string, 1000),
		workerCount: workers,
		delay:       delay,
		domain:      parsed.Host,
		outputDir:   outputDir,
	}, nil
}

// Start å¯åŠ¨çˆ¬è™«
func (c *Crawler) Start() {
	fmt.Printf("ğŸš€ å¼€å§‹çˆ¬å–: %s\n", c.startURL)
	fmt.Printf("ğŸ“ è¾“å‡ºç›®å½•: %s\n", c.outputDir)

	var wg sync.WaitGroup

	// å¯åŠ¨ç»“æœå¤„ç†å™¨
	wg.Add(1)
	go c.resultProcessor(&wg)

	// å¯åŠ¨worker
	urlChan := make(chan URLInfo, 1000)
	for i := 0; i < c.workerCount; i++ {
		wg.Add(1)
		go c.worker(i+1, urlChan, &wg)
	}

	// å‘é€åˆå§‹URL
	urlChan <- URLInfo{URL: c.startURL, Depth: 0}

	// ç­‰å¾…æ‰€æœ‰workerå®Œæˆ
	close(urlChan)
	wg.Wait()
	close(c.results)

	fmt.Println("\nâœ… çˆ¬å–å®Œæˆ!")
}

// worker çˆ¬è™«å·¥ä½œçº¿ç¨‹
func (c *Crawler) worker(id int, urlChan chan URLInfo, wg *sync.WaitGroup) {
	defer wg.Done()

	for task := range urlChan {
		// æ£€æŸ¥æ·±åº¦é™åˆ¶
		if task.Depth > c.maxDepth {
			continue
		}

		// æ£€æŸ¥æ˜¯å¦å·²è®¿é—®
		if _, visited := c.visited.LoadOrStore(task.URL, true); visited {
			continue
		}

		// å»¶è¿Ÿæ§åˆ¶
		time.Sleep(c.delay)

		// è·å–é¡µé¢
		fmt.Printf("[Worker %d] ğŸ“¥ è·å–: %s (æ·±åº¦: %d)\n",
			id, task.URL, task.Depth)

		content, links, err := c.fetchPage(task.URL)
		if err != nil {
			fmt.Printf("[Worker %d] âŒ é”™è¯¯: %s - %v\n",
				id, task.URL, err)
			continue
		}

		// ä¿å­˜é¡µé¢å†…å®¹
		c.results <- fmt.Sprintf("PAGE|%s|%s", task.URL, content)

		// æå–å¹¶å¤„ç†é“¾æ¥
		for _, link := range links {
			// è½¬æ¢ä¸ºç»å¯¹URL
			absoluteURL := c.resolveURL(task.URL, link)
			if absoluteURL == "" {
				continue
			}

			// æ£€æŸ¥åŸŸåé™åˆ¶ï¼ˆå¯é€‰ï¼‰
			if !c.isSameDomain(absoluteURL) {
				continue
			}

			// å‘é€åˆ°é˜Ÿåˆ—
			select {
			case urlChan <- URLInfo{URL: absoluteURL, Depth: task.Depth + 1}:
			default:
				// é˜Ÿåˆ—æ»¡ï¼Œä¸¢å¼ƒ
			}
		}
	}
}

// fetchPage è·å–é¡µé¢å†…å®¹å’Œé“¾æ¥
func (c *Crawler) fetchPage(urlStr string) (string, []string, error) {
	client := &http.Client{
		Timeout: 10 * time.Second,
	}

	req, err := http.NewRequest("GET", urlStr, nil)
	if err != nil {
		return "", nil, err
	}

	// è®¾ç½®User-Agent
	req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")

	resp, err := client.Do(req)
	if err != nil {
		return "", nil, err
	}
	defer resp.BodyClose()

	if resp.StatusCode != http.StatusOK {
		return "", nil, fmt.Errorf("HTTP %d", resp.StatusCode)
	}

	// è¯»å–å†…å®¹
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", nil, err
	}

	// æå–é“¾æ¥
	links := c.extractLinks(urlStr, body)

	return string(body), links, nil
}

// extractLinks ä»HTMLä¸­æå–é“¾æ¥
func (c *Crawler) extractLinks(baseURL string, content []byte) []string {
	var links []string

	doc, err := html.Parse(bytes.NewReader(content))
	if err != nil {
		return links
	}

	var f func(*html.Node)
	f = func(n *html.Node) {
		if n.Type == html.ElementNode {
			var attrName string

			switch n.Data {
			case "a", "link":
				attrName = "href"
			case "img", "script":
				attrName = "src"
			case "iframe":
				attrName = "src"
			}

			if attrName != "" {
				for _, attr := range n.Attr {
					if attr.Key == attrName && attr.Val != "" {
						links = append(links, attr.Val)
					}
				}
			}
		}

		for child := n.FirstChild; child != nil; child = child.NextSibling {
			f(child)
		}
	}

	f(doc)
	return links
}

// resolveURL è§£æç›¸å¯¹URLä¸ºç»å¯¹URL
func (c *Crawler) resolveURL(base, relative string) string {
	baseURL, err := url.Parse(base)
	if err != nil {
		return ""
	}

	relURL, err := url.Parse(relative)
	if err != nil {
		return ""
	}

	absURL := baseURL.ResolveReference(relURL)
	return absURL.String()
}

// isSameDomain æ£€æŸ¥æ˜¯å¦åŒä¸€åŸŸå
func (c *Crawler) isSameDomain(urlStr string) bool {
	parsed, err := url.Parse(urlStr)
	if err != nil {
		return false
	}
	return parsed.Host == c.domain || parsed.Host == ""
}

// resultProcessor å¤„ç†çˆ¬å–ç»“æœ
func (c *Crawler) resultProcessor(wg *sync.WaitGroup) {
	defer wg.Done()

	// åˆ›å»ºç´¢å¼•æ–‡ä»¶
	indexFile, err := os.Create(filepath.Join(c.outputDir, "index.txt"))
	if err != nil {
		fmt.Printf("âŒ åˆ›å»ºç´¢å¼•æ–‡ä»¶å¤±è´¥: %v\n", err)
		return
	}
	defer indexFile.Close()

	for result := range c.results {
		parts := strings.SplitN(result, "|", 3)
		if len(parts) != 3 {
			continue
		}

		urlStr := parts[1]
		content := parts[2]

		// ä¿å­˜åˆ°æ–‡ä»¶
		filename := c.generateFilename(urlStr)
		filepath := filepath.Join(c.outputDir, filename)

		if err := os.WriteFile(filepath, []byte(content), 0644); err != nil {
			fmt.Printf("âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: %s - %v\n", urlStr, err)
			continue
		}

		// è®°å½•åˆ°ç´¢å¼•
		indexFile.WriteString(fmt.Sprintf("%s -> %s\n", urlStr, filename))

		fmt.Printf("ğŸ’¾ å·²ä¿å­˜: %s\n", filename)
	}
}

// generateFilename ç”Ÿæˆæ–‡ä»¶å
func (c *Crawler) generateFilename(urlStr string) string {
	// ç§»é™¤åè®®å’Œéæ³•å­—ç¬¦
	filename := strings.ReplaceAll(urlStr, "://", "_")
	filename = strings.ReplaceAll(filename, "/", "_")
	filename = strings.ReplaceAll(filename, "?", "_")
	filename = strings.ReplaceAll(filename, "&", "_")
	filename = strings.ReplaceAll(filename, "=", "_")

	// é™åˆ¶é•¿åº¦
	if len(filename) > 100 {
		filename = filename[:100]
	}

	return filename + ".html"
}

func main() {
	startURL := "https://example.com"
	maxDepth := 2
	workers := 5
	delay := 1 * time.Second

	crawler, err := NewCrawler(startURL, maxDepth, workers, delay)
	if err != nil {
		fmt.Printf("âŒ åˆå§‹åŒ–å¤±è´¥: %v\n", err)
		return
	}

	crawler.Start()
}
